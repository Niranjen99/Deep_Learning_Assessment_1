{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkEtpYniD1ZodsvgSsb8eF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niranjen99/Deep_Learning_Assessment_1/blob/main/MLPDiabetes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1d3USObO3M8f",
        "outputId": "0fc5c82b-ac21-46a1-963a-fbb52566c82a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.  1. -1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1.\n",
            "  1. -1.  1.  1. -1. -1. -1. -1. -1.  1.  1.  1.  1. -1.  1.  1.  1.  1.\n",
            "  1. -1. -1. -1.  1.  1.  1. -1.  1. -1.  1.  1. -1.  1.  1.  1.  1. -1.\n",
            "  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1.\n",
            " -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.\n",
            "  1.  1.  1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
            "  1. -1. -1. -1.  1.  1. -1. -1. -1.  1.  1.  1. -1.  1.  1.  1. -1. -1.\n",
            "  1.  1. -1. -1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
            "  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1.  1.  1.  1. -1.  1.  1.\n",
            "  1.  1. -1. -1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1. -1.  1. -1.\n",
            "  1.  1.  1.  1.  1. -1. -1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1.\n",
            " -1. -1.  1.  1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1. -1.\n",
            " -1.  1. -1. -1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1.  1.\n",
            "  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
            "  1.  1. -1. -1.  1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.\n",
            " -1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1. -1. -1.  1.  1. -1.\n",
            "  1.  1.  1. -1. -1. -1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1.  1.\n",
            " -1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1. -1. -1. -1.\n",
            "  1.  1. -1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1.\n",
            "  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1. -1. -1. -1.  1. -1.\n",
            " -1.  1.  1. -1.  1.  1. -1.  1.  1. -1. -1.  1.  1.  1.  1. -1.  1.  1.\n",
            " -1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1.\n",
            "  1. -1.  1. -1. -1.  1. -1.  1. -1.  1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
            " -1. -1.  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
            "  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1.  1.  1. -1.  1.\n",
            "  1. -1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
            " -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1.  1. -1. -1.\n",
            "  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.\n",
            "  1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.\n",
            "  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.\n",
            " -1. -1. -1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
            "  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
            "  1. -1.  1. -1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1.\n",
            "  1. -1.  1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1.  1.  1. -1.\n",
            " -1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
            " -1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1. -1.  1.  1.  1. -1. -1.\n",
            " -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1. -1. -1. -1. -1.  1.\n",
            " -1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1. -1.\n",
            "  1.  1.  1.  1.  1. -1.  1. -1.  1. -1.  1. -1. -1.  1.  1.  1.  1. -1.\n",
            " -1.  1.  1.  1. -1.  1. -1. -1.  1.  1. -1.  1.  1. -1. -1.  1.  1. -1.\n",
            "  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1.\n",
            "  1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1.\n",
            "  1. -1.  1. -1.  1. -1.  1.  1.  1.  1. -1.  1.]\n",
            "Feature matrix shape: (768, 8)\n",
            "Labels shape: (768,)\n",
            "[0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "# Download the libsvm data from a URL\n",
        "#url = 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/diabetes_scale'\n",
        "url = 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/diabetes'\n",
        "\n",
        "data_file = 'data.libsvm'\n",
        "\n",
        "# Download the data from the URL\n",
        "urllib.request.urlretrieve(url, data_file)\n",
        "\n",
        "# Load the libsvm data using scikit-learn\n",
        "X, y = load_svmlight_file(data_file)\n",
        "\n",
        "# Convert sparse matrix to a dense NumPy array (if necessary)\n",
        "X = X.toarray()\n",
        "\n",
        "print(y)\n",
        "#change label from -1 to 0 and 1 to 1\n",
        "y = np.where(y == -1, 0, 1)\n",
        "\n",
        "\n",
        "# Print shape of the data to confirm\n",
        "print(\"Feature matrix shape:\", X.shape)\n",
        "print(\"Labels shape:\", y.shape)\n",
        "#print(X)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the scaler and scale features between -1 and 1\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "print(X_train)\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer and first hidden layer with ReLU activation\n",
        "model.add(Dense(32, input_dim=8, activation='relu'))\n",
        "\n",
        "# Second hidden layer with ReLU activation\n",
        "model.add(Dense(16, activation='relu'))\n",
        "\n",
        "model.add(Dense(8, activation='relu'))\n",
        "\n",
        "\n",
        "\n",
        "# Output layer with tanh activation since labels  are classifying between -1 and 1\n",
        "#model.add(Dense(1, activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "#model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['accuracy'])\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "XP4bViVGYk80",
        "outputId": "608ff056-3c18-4dca-aeb2-218f290b0bf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.11764706 0.42211055 0.         ... 0.         0.09649872 0.        ]\n",
            " [0.52941176 0.56281407 0.67213115 ... 0.42026828 0.51409052 0.48333333]\n",
            " [0.05882353 0.69849246 0.37704918 ... 0.42771985 0.24594364 0.01666667]\n",
            " ...\n",
            " [0.58823529 0.50753769 0.70491803 ... 0.6795827  0.45175064 0.28333333]\n",
            " [0.         0.70854271 0.         ... 0.63189275 0.05422716 0.13333333]\n",
            " [0.         0.6281407  0.78688525 ... 0.33532043 0.07856533 0.        ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "#model.fit(X_train, y_train, epochs=100, batch_size=16, verbose=1)\n",
        "hist = model.fit(X_train,y_train, epochs =100,validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on training data\n",
        "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
        "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "m6_ycVnHYt0h",
        "outputId": "6f643fe4-c68d-47e3-8d00-b37bc88e546b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8035 - loss: 0.4061 - val_accuracy: 0.7792 - val_loss: 0.5214\n",
            "Epoch 2/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8325 - loss: 0.3603 - val_accuracy: 0.7403 - val_loss: 0.5314\n",
            "Epoch 3/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8218 - loss: 0.3953 - val_accuracy: 0.7792 - val_loss: 0.5218\n",
            "Epoch 4/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8051 - loss: 0.3946 - val_accuracy: 0.7792 - val_loss: 0.5231\n",
            "Epoch 5/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7870 - loss: 0.4312 - val_accuracy: 0.7597 - val_loss: 0.5236\n",
            "Epoch 6/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7956 - loss: 0.4121 - val_accuracy: 0.7857 - val_loss: 0.5259\n",
            "Epoch 7/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8371 - loss: 0.3649 - val_accuracy: 0.7338 - val_loss: 0.5330\n",
            "Epoch 8/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8360 - loss: 0.3718 - val_accuracy: 0.7857 - val_loss: 0.5258\n",
            "Epoch 9/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8215 - loss: 0.3788 - val_accuracy: 0.7403 - val_loss: 0.5340\n",
            "Epoch 10/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8217 - loss: 0.3929 - val_accuracy: 0.7662 - val_loss: 0.5247\n",
            "Epoch 11/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8180 - loss: 0.3918 - val_accuracy: 0.7792 - val_loss: 0.5205\n",
            "Epoch 12/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8026 - loss: 0.3929 - val_accuracy: 0.7403 - val_loss: 0.5303\n",
            "Epoch 13/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8277 - loss: 0.3729 - val_accuracy: 0.7792 - val_loss: 0.5346\n",
            "Epoch 14/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7968 - loss: 0.4201 - val_accuracy: 0.7403 - val_loss: 0.5295\n",
            "Epoch 15/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8149 - loss: 0.4004 - val_accuracy: 0.7662 - val_loss: 0.5232\n",
            "Epoch 16/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8098 - loss: 0.4109 - val_accuracy: 0.7662 - val_loss: 0.5207\n",
            "Epoch 17/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8330 - loss: 0.3625 - val_accuracy: 0.7468 - val_loss: 0.5238\n",
            "Epoch 18/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8269 - loss: 0.3862 - val_accuracy: 0.7792 - val_loss: 0.5366\n",
            "Epoch 19/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8005 - loss: 0.4063 - val_accuracy: 0.7403 - val_loss: 0.5251\n",
            "Epoch 20/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8228 - loss: 0.3895 - val_accuracy: 0.7532 - val_loss: 0.5203\n",
            "Epoch 21/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8011 - loss: 0.4204 - val_accuracy: 0.7857 - val_loss: 0.5282\n",
            "Epoch 22/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8296 - loss: 0.3697 - val_accuracy: 0.7597 - val_loss: 0.5275\n",
            "Epoch 23/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8211 - loss: 0.3891 - val_accuracy: 0.7792 - val_loss: 0.5224\n",
            "Epoch 24/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8248 - loss: 0.3897 - val_accuracy: 0.7597 - val_loss: 0.5276\n",
            "Epoch 25/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8274 - loss: 0.3836 - val_accuracy: 0.7727 - val_loss: 0.5239\n",
            "Epoch 26/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8271 - loss: 0.3850 - val_accuracy: 0.7403 - val_loss: 0.5261\n",
            "Epoch 27/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8366 - loss: 0.3805 - val_accuracy: 0.7662 - val_loss: 0.5263\n",
            "Epoch 28/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8335 - loss: 0.3948 - val_accuracy: 0.7662 - val_loss: 0.5236\n",
            "Epoch 29/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8321 - loss: 0.3789 - val_accuracy: 0.7403 - val_loss: 0.5295\n",
            "Epoch 30/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8453 - loss: 0.3806 - val_accuracy: 0.7597 - val_loss: 0.5280\n",
            "Epoch 31/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8418 - loss: 0.3810 - val_accuracy: 0.7532 - val_loss: 0.5274\n",
            "Epoch 32/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8338 - loss: 0.3856 - val_accuracy: 0.7597 - val_loss: 0.5253\n",
            "Epoch 33/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8358 - loss: 0.3691 - val_accuracy: 0.7532 - val_loss: 0.5259\n",
            "Epoch 34/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8220 - loss: 0.4023 - val_accuracy: 0.7403 - val_loss: 0.5340\n",
            "Epoch 35/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8175 - loss: 0.4079 - val_accuracy: 0.7597 - val_loss: 0.5308\n",
            "Epoch 36/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8328 - loss: 0.3978 - val_accuracy: 0.7597 - val_loss: 0.5275\n",
            "Epoch 37/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8262 - loss: 0.4069 - val_accuracy: 0.7338 - val_loss: 0.5388\n",
            "Epoch 38/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8141 - loss: 0.3990 - val_accuracy: 0.7468 - val_loss: 0.5298\n",
            "Epoch 39/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8322 - loss: 0.3882 - val_accuracy: 0.7468 - val_loss: 0.5311\n",
            "Epoch 40/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8348 - loss: 0.3665 - val_accuracy: 0.7792 - val_loss: 0.5287\n",
            "Epoch 41/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8282 - loss: 0.3717 - val_accuracy: 0.7403 - val_loss: 0.5312\n",
            "Epoch 42/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8267 - loss: 0.3826 - val_accuracy: 0.7727 - val_loss: 0.5423\n",
            "Epoch 43/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8423 - loss: 0.3588 - val_accuracy: 0.7597 - val_loss: 0.5399\n",
            "Epoch 44/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8346 - loss: 0.3750 - val_accuracy: 0.7338 - val_loss: 0.5330\n",
            "Epoch 45/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8164 - loss: 0.3919 - val_accuracy: 0.7403 - val_loss: 0.5355\n",
            "Epoch 46/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8272 - loss: 0.4055 - val_accuracy: 0.7662 - val_loss: 0.5600\n",
            "Epoch 47/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8064 - loss: 0.4039 - val_accuracy: 0.7403 - val_loss: 0.5396\n",
            "Epoch 48/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8048 - loss: 0.3795 - val_accuracy: 0.7338 - val_loss: 0.5357\n",
            "Epoch 49/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7947 - loss: 0.4058 - val_accuracy: 0.7662 - val_loss: 0.5362\n",
            "Epoch 50/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8478 - loss: 0.3465 - val_accuracy: 0.7468 - val_loss: 0.5387\n",
            "Epoch 51/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8570 - loss: 0.3540 - val_accuracy: 0.7597 - val_loss: 0.5310\n",
            "Epoch 52/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8134 - loss: 0.3883 - val_accuracy: 0.7597 - val_loss: 0.5283\n",
            "Epoch 53/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8337 - loss: 0.3673 - val_accuracy: 0.7468 - val_loss: 0.5335\n",
            "Epoch 54/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8188 - loss: 0.3955 - val_accuracy: 0.7597 - val_loss: 0.5396\n",
            "Epoch 55/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8293 - loss: 0.3784 - val_accuracy: 0.7403 - val_loss: 0.5301\n",
            "Epoch 56/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8304 - loss: 0.3954 - val_accuracy: 0.7338 - val_loss: 0.5364\n",
            "Epoch 57/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8288 - loss: 0.3863 - val_accuracy: 0.7468 - val_loss: 0.5386\n",
            "Epoch 58/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8395 - loss: 0.3575 - val_accuracy: 0.7468 - val_loss: 0.5292\n",
            "Epoch 59/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8290 - loss: 0.3769 - val_accuracy: 0.7532 - val_loss: 0.5315\n",
            "Epoch 60/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8143 - loss: 0.3843 - val_accuracy: 0.7468 - val_loss: 0.5370\n",
            "Epoch 61/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8229 - loss: 0.3784 - val_accuracy: 0.7597 - val_loss: 0.5414\n",
            "Epoch 62/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8257 - loss: 0.3700 - val_accuracy: 0.7532 - val_loss: 0.5397\n",
            "Epoch 63/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8400 - loss: 0.3422 - val_accuracy: 0.7468 - val_loss: 0.5379\n",
            "Epoch 64/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8299 - loss: 0.3886 - val_accuracy: 0.7532 - val_loss: 0.5414\n",
            "Epoch 65/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8501 - loss: 0.3544 - val_accuracy: 0.7403 - val_loss: 0.5364\n",
            "Epoch 66/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8380 - loss: 0.3691 - val_accuracy: 0.7727 - val_loss: 0.5474\n",
            "Epoch 67/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8278 - loss: 0.3614 - val_accuracy: 0.7532 - val_loss: 0.5383\n",
            "Epoch 68/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8313 - loss: 0.3604 - val_accuracy: 0.7597 - val_loss: 0.5328\n",
            "Epoch 69/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8594 - loss: 0.3416 - val_accuracy: 0.7532 - val_loss: 0.5342\n",
            "Epoch 70/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8307 - loss: 0.3633 - val_accuracy: 0.7532 - val_loss: 0.5311\n",
            "Epoch 71/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8281 - loss: 0.3651 - val_accuracy: 0.7403 - val_loss: 0.5416\n",
            "Epoch 72/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8346 - loss: 0.3719 - val_accuracy: 0.7532 - val_loss: 0.5373\n",
            "Epoch 73/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8431 - loss: 0.3770 - val_accuracy: 0.7468 - val_loss: 0.5303\n",
            "Epoch 74/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8498 - loss: 0.3463 - val_accuracy: 0.7532 - val_loss: 0.5385\n",
            "Epoch 75/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8432 - loss: 0.3583 - val_accuracy: 0.7403 - val_loss: 0.5312\n",
            "Epoch 76/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8308 - loss: 0.3791 - val_accuracy: 0.7468 - val_loss: 0.5409\n",
            "Epoch 77/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8578 - loss: 0.3601 - val_accuracy: 0.7532 - val_loss: 0.5374\n",
            "Epoch 78/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8357 - loss: 0.3839 - val_accuracy: 0.7597 - val_loss: 0.5519\n",
            "Epoch 79/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8240 - loss: 0.3887 - val_accuracy: 0.7532 - val_loss: 0.5417\n",
            "Epoch 80/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8185 - loss: 0.3765 - val_accuracy: 0.7208 - val_loss: 0.5417\n",
            "Epoch 81/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8373 - loss: 0.3466 - val_accuracy: 0.7532 - val_loss: 0.5429\n",
            "Epoch 82/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8252 - loss: 0.4194 - val_accuracy: 0.7597 - val_loss: 0.5451\n",
            "Epoch 83/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8575 - loss: 0.3523 - val_accuracy: 0.7468 - val_loss: 0.5404\n",
            "Epoch 84/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8434 - loss: 0.3612 - val_accuracy: 0.7532 - val_loss: 0.5351\n",
            "Epoch 85/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8481 - loss: 0.3572 - val_accuracy: 0.7468 - val_loss: 0.5463\n",
            "Epoch 86/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8512 - loss: 0.3577 - val_accuracy: 0.7532 - val_loss: 0.5364\n",
            "Epoch 87/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8347 - loss: 0.3740 - val_accuracy: 0.7468 - val_loss: 0.5565\n",
            "Epoch 88/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8566 - loss: 0.3355 - val_accuracy: 0.7597 - val_loss: 0.5367\n",
            "Epoch 89/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8399 - loss: 0.3639 - val_accuracy: 0.7532 - val_loss: 0.5383\n",
            "Epoch 90/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8420 - loss: 0.3560 - val_accuracy: 0.7468 - val_loss: 0.5430\n",
            "Epoch 91/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8461 - loss: 0.3462 - val_accuracy: 0.7468 - val_loss: 0.5455\n",
            "Epoch 92/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8140 - loss: 0.3816 - val_accuracy: 0.7597 - val_loss: 0.5387\n",
            "Epoch 93/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8436 - loss: 0.3635 - val_accuracy: 0.7532 - val_loss: 0.5389\n",
            "Epoch 94/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8295 - loss: 0.3825 - val_accuracy: 0.7532 - val_loss: 0.5423\n",
            "Epoch 95/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8421 - loss: 0.3543 - val_accuracy: 0.7468 - val_loss: 0.5474\n",
            "Epoch 96/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8387 - loss: 0.3546 - val_accuracy: 0.7532 - val_loss: 0.5435\n",
            "Epoch 97/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8306 - loss: 0.3759 - val_accuracy: 0.7468 - val_loss: 0.5487\n",
            "Epoch 98/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8406 - loss: 0.3654 - val_accuracy: 0.7468 - val_loss: 0.5540\n",
            "Epoch 99/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8575 - loss: 0.3374 - val_accuracy: 0.7468 - val_loss: 0.5469\n",
            "Epoch 100/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8230 - loss: 0.3759 - val_accuracy: 0.7468 - val_loss: 0.5457\n",
            "Training Accuracy: 84.20%\n",
            "Test Accuracy: 74.68%\n"
          ]
        }
      ]
    }
  ]
}